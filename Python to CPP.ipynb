{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kOXYulq9z9M4",
      "metadata": {
        "id": "kOXYulq9z9M4"
      },
      "outputs": [],
      "source": [
        "!pip install openai google huggingface_hub google-genai gradio transformers torch bitsandbytes requests accelerate sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e63c1c9c",
      "metadata": {
        "id": "e63c1c9c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import sys\n",
        "from openai import OpenAI\n",
        "from google import genai\n",
        "from IPython.display import Markdown, display, update_display\n",
        "import gradio as gr\n",
        "import subprocess\n",
        "import platform\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig, TextStreamer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a97e767",
      "metadata": {
        "id": "8a97e767"
      },
      "outputs": [],
      "source": [
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)\n",
        "\n",
        "google_key = userdata.get('GOOGLE_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_8vKFPhuz8dD",
      "metadata": {
        "id": "_8vKFPhuz8dD"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are an assistant that reimplements Python code in high performance C++ for an M1 Mac. \"\n",
        "system_message += \"Respond only with C++ code; use comments sparingly and do not provide any explanation other than occasional comments. \"\n",
        "system_message += \"The C++ response needs to produce an identical output in the fastest possible time. Keep implementations of random number generators identical so that results match exactly.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SKkGkNS3z8dD",
      "metadata": {
        "id": "SKkGkNS3z8dD"
      },
      "outputs": [],
      "source": [
        "def user_prompt(python):\n",
        "    user_prompt = \"Rewrite this Python code in C++ with the fastest possible implementation that produces identical output in the least time. \"\n",
        "    user_prompt += \"Respond only with C++ code; do not explain your work other than a few comments. \"\n",
        "    user_prompt += \"Pay attention to number types to ensure no int overflows. Remember to #include all necessary C++ packages such as iomanip.\\n\\n\"\n",
        "    user_prompt += python\n",
        "    return user_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84426cfa",
      "metadata": {
        "id": "84426cfa"
      },
      "outputs": [],
      "source": [
        "def messages_for(python):\n",
        "    return [\n",
        "        {\"role\": \"system\", \"content\": system_message},\n",
        "        {\"role\": \"user\", \"content\": user_prompt(python)}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60ec7b30",
      "metadata": {
        "id": "60ec7b30"
      },
      "outputs": [],
      "source": [
        "pi = \"\"\"\n",
        "import time\n",
        "\n",
        "def calculate(iterations, param1, param2):\n",
        "    result = 1.0\n",
        "    for i in range(1, iterations+1):\n",
        "        j = i * param1 - param2\n",
        "        result -= (1/j)\n",
        "        j = i * param1 + param2\n",
        "        result += (1/j)\n",
        "    return result\n",
        "\n",
        "start_time = time.time()\n",
        "result = calculate(100_000_000, 4, 1) * 4\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Result: {result:.12f}\")\n",
        "print(f\"Execution Time: {(end_time - start_time):.6f} seconds\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-oCcmv11JDu",
      "metadata": {
        "id": "H-oCcmv11JDu"
      },
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fwffaGmY1Uy-",
      "metadata": {
        "id": "fwffaGmY1Uy-"
      },
      "outputs": [],
      "source": [
        "NxCode = 'NTQAI/Nxcode-CQ-7B-orpo'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fBE0khsw1mnF",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fBE0khsw1mnF"
      },
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(NxCode)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "messages = messages_for(pi)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    NxCode,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E0XHEcor2TOf",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "E0XHEcor2TOf"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def generate_code_NxCode(model, tokenizer, messages):\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors='pt',add_generation_prompt=False).to('cuda')\n",
        "    streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract C++ code block between ```cpp ... ```\n",
        "    match = re.search(r\"```cpp(.*?)```\", response, re.DOTALL)\n",
        "    if match:\n",
        "        cpp_code = match.group(1).strip()\n",
        "    else:\n",
        "        cpp_code = \"C++ code not found.\"\n",
        "\n",
        "    return cpp_code\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-CqLgZNcHR11",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-CqLgZNcHR11"
      },
      "outputs": [],
      "source": [
        "cpp_code_NxCode = generate_code_NxCode(model, tokenizer, messages)\n",
        "print(cpp_code_NxCode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UMGe0w67YtRt",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UMGe0w67YtRt"
      },
      "outputs": [],
      "source": [
        "def extract_cpp_code(python):\n",
        "    messages = messages_for(python)\n",
        "    inputs = tokenizer.apply_chat_template(messages, return_tensors='pt', add_generation_prompt=False).to('cuda')\n",
        "    outputs = model.generate(inputs, max_new_tokens=2000)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    match = re.search(r\"```cpp(.*?)```\", response, re.DOTALL)\n",
        "    return match.group(1).strip() if match else \"C++ code not found.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7SYqaeZIZNT8",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7SYqaeZIZNT8"
      },
      "outputs": [],
      "source": [
        "extract_cpp_code(pi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0BtLaQvdXqEK",
      "metadata": {
        "id": "0BtLaQvdXqEK"
      },
      "outputs": [],
      "source": [
        "with gr.Blocks() as ui :\n",
        "  gr.Markdown('# Convert Python to C++')\n",
        "\n",
        "  with gr.Row():\n",
        "      python = gr.Textbox(label=\"Python code:\", value=pi, lines=10)\n",
        "      cpp = gr.Textbox(label=\"C++ code:\", lines=10)\n",
        "\n",
        "  with gr.Row():\n",
        "      convert = gr.Button(\"Convert code\")\n",
        "\n",
        "  convert.click(extract_cpp_code, inputs=[python], outputs=[cpp])\n",
        "\n",
        "ui.launch(share=True,debug=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}